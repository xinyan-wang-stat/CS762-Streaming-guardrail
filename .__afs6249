Random seed set globally to 42
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [02:50<11:21, 170.37s/it]Loading checkpoint shards:  40%|████      | 2/5 [05:29<08:11, 163.91s/it]Loading checkpoint shards:  60%|██████    | 3/5 [08:09<05:23, 161.83s/it]Loading checkpoint shards:  80%|████████  | 4/5 [10:28<02:32, 152.91s/it]Loading checkpoint shards: 100%|██████████| 5/5 [11:23<00:00, 117.73s/it]Loading checkpoint shards: 100%|██████████| 5/5 [11:23<00:00, 136.74s/it]
Building per-sample cache into data/seval_qwen3_8b_dataset/train/safety_cache/-home-ruqi-public-Qwen3-8B/idx32_maxlength1024 ...
Build samples:   0%|          | 0/80 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Build samples:   1%|▏         | 1/80 [00:17<23:10, 17.60s/it]Build samples:   2%|▎         | 2/80 [00:20<11:23,  8.76s/it]Build samples:   4%|▍         | 3/80 [00:22<07:40,  5.99s/it]Build samples:   5%|▌         | 4/80 [00:23<05:04,  4.01s/it]Build samples:   6%|▋         | 5/80 [00:24<03:26,  2.76s/it]Build samples:   8%|▊         | 6/80 [00:25<02:32,  2.07s/it]Build samples:   9%|▉         | 7/80 [00:26<02:20,  1.93s/it]Build samples:  10%|█         | 8/80 [00:32<03:50,  3.20s/it]Build samples:  11%|█▏        | 9/80 [00:33<02:50,  2.40s/it]Build samples:  12%|█▎        | 10/80 [00:34<02:29,  2.14s/it]Build samples:  14%|█▍        | 11/80 [00:35<01:53,  1.65s/it]Build samples:  15%|█▌        | 12/80 [00:37<02:01,  1.79s/it]Build samples:  16%|█▋        | 13/80 [00:39<01:59,  1.78s/it]Build samples:  18%|█▊        | 14/80 [00:41<01:57,  1.77s/it]Build samples:  19%|█▉        | 15/80 [00:44<02:20,  2.16s/it]Build samples:  20%|██        | 16/80 [00:45<01:59,  1.86s/it]Build samples:  21%|██▏       | 17/80 [00:48<02:26,  2.33s/it]Build samples:  22%|██▎       | 18/80 [00:50<02:24,  2.33s/it]Build samples:  24%|██▍       | 19/80 [00:52<01:59,  1.97s/it]Build samples:  25%|██▌       | 20/80 [00:53<01:54,  1.91s/it]Build samples:  26%|██▋       | 21/80 [00:57<02:18,  2.35s/it]Build samples:  28%|██▊       | 22/80 [01:00<02:23,  2.47s/it]Build samples:  29%|██▉       | 23/80 [01:01<02:03,  2.17s/it]Build samples:  30%|███       | 24/80 [01:02<01:39,  1.78s/it]Build samples:  31%|███▏      | 25/80 [01:05<02:00,  2.19s/it]Build samples:  32%|███▎      | 26/80 [01:06<01:32,  1.72s/it]Build samples:  34%|███▍      | 27/80 [01:06<01:13,  1.38s/it]Build samples:  35%|███▌      | 28/80 [01:08<01:21,  1.56s/it]Build samples:  36%|███▋      | 29/80 [01:10<01:25,  1.68s/it]Build samples:  38%|███▊      | 30/80 [01:12<01:30,  1.81s/it]Build samples:  39%|███▉      | 31/80 [01:15<01:35,  1.96s/it]Build samples:  40%|████      | 32/80 [01:16<01:21,  1.71s/it]Build samples:  41%|████▏     | 33/80 [01:17<01:20,  1.72s/it]Build samples:  42%|████▎     | 34/80 [01:18<01:09,  1.51s/it]Build samples:  44%|████▍     | 35/80 [01:21<01:15,  1.69s/it]Build samples:  45%|████▌     | 36/80 [01:21<00:59,  1.35s/it]Build samples:  46%|████▋     | 37/80 [01:25<01:24,  1.96s/it]Build samples:  48%|████▊     | 38/80 [01:27<01:24,  2.02s/it]Build samples:  49%|████▉     | 39/80 [01:28<01:09,  1.69s/it]Build samples:  50%|█████     | 40/80 [01:28<00:52,  1.31s/it]Build samples:  51%|█████▏    | 41/80 [01:29<00:42,  1.09s/it]Build samples:  52%|█████▎    | 42/80 [01:30<00:39,  1.04s/it]Build samples:  54%|█████▍    | 43/80 [01:31<00:45,  1.22s/it]Build samples:  55%|█████▌    | 44/80 [01:35<01:08,  1.91s/it]Build samples:  56%|█████▋    | 45/80 [01:36<01:05,  1.87s/it]Build samples:  57%|█████▊    | 46/80 [01:37<00:53,  1.57s/it]Build samples:  59%|█████▉    | 47/80 [01:39<00:54,  1.64s/it]Build samples:  60%|██████    | 48/80 [01:41<00:50,  1.57s/it]Build samples:  61%|██████▏   | 49/80 [01:43<01:01,  1.98s/it]Build samples:  62%|██████▎   | 50/80 [01:44<00:45,  1.52s/it]Build samples:  64%|██████▍   | 51/80 [01:45<00:37,  1.31s/it]Build samples:  65%|██████▌   | 52/80 [01:46<00:35,  1.26s/it]Build samples:  66%|██████▋   | 53/80 [01:47<00:29,  1.08s/it]Build samples:  68%|██████▊   | 54/80 [01:48<00:34,  1.32s/it]Build samples:  69%|██████▉   | 55/80 [01:49<00:26,  1.05s/it]Build samples:  70%|███████   | 56/80 [01:52<00:37,  1.54s/it]Build samples:  71%|███████▏  | 57/80 [01:52<00:27,  1.21s/it]Build samples:  72%|███████▎  | 58/80 [01:55<00:38,  1.77s/it]Build samples:  74%|███████▍  | 59/80 [01:55<00:28,  1.36s/it]Build samples:  75%|███████▌  | 60/80 [01:57<00:30,  1.53s/it]Build samples:  76%|███████▋  | 61/80 [02:01<00:40,  2.15s/it]Build samples:  78%|███████▊  | 62/80 [02:04<00:42,  2.38s/it]Build samples:  79%|███████▉  | 63/80 [02:06<00:40,  2.41s/it]Build samples:  80%|████████  | 64/80 [02:10<00:43,  2.70s/it]Build samples:  81%|████████▏ | 65/80 [02:11<00:35,  2.38s/it]Build samples:  82%|████████▎ | 66/80 [02:14<00:33,  2.36s/it]Build samples:  84%|████████▍ | 67/80 [02:15<00:28,  2.19s/it]Build samples:  85%|████████▌ | 68/80 [02:18<00:27,  2.31s/it]Build samples:  86%|████████▋ | 69/80 [02:21<00:27,  2.53s/it]Build samples:  88%|████████▊ | 70/80 [02:24<00:26,  2.65s/it]Build samples:  89%|████████▉ | 71/80 [02:28<00:26,  2.99s/it]Build samples:  90%|█████████ | 72/80 [02:28<00:18,  2.29s/it]Build samples:  91%|█████████▏| 73/80 [02:30<00:14,  2.12s/it]Build samples:  92%|█████████▎| 74/80 [02:33<00:13,  2.31s/it]Build samples:  94%|█████████▍| 75/80 [02:35<00:11,  2.27s/it]Build samples:  95%|█████████▌| 76/80 [02:39<00:10,  2.74s/it]Build samples:  96%|█████████▋| 77/80 [02:42<00:08,  2.87s/it]Build samples:  98%|█████████▊| 78/80 [02:44<00:05,  2.68s/it]Build samples:  99%|█████████▉| 79/80 [02:47<00:02,  2.65s/it]Build samples: 100%|██████████| 80/80 [02:50<00:00,  2.74s/it]Build samples: 100%|██████████| 80/80 [02:50<00:00,  2.13s/it]
Cache build finished at data/seval_qwen3_8b_dataset/train/safety_cache/-home-ruqi-public-Qwen3-8B/idx32_maxlength1024
Total trainable parameters:  19.933186 M
Epoch 1/1:   0%|          | 0/80 [00:00<?, ?it/s]Epoch 1/1:   1%|▏         | 1/80 [00:07<10:02,  7.62s/it]Epoch 1/1:   2%|▎         | 2/80 [00:08<04:42,  3.62s/it]Epoch 1/1:   4%|▍         | 3/80 [00:09<02:58,  2.32s/it]Epoch 1/1:   5%|▌         | 4/80 [00:09<01:59,  1.57s/it]Epoch 1/1:   6%|▋         | 5/80 [00:11<01:55,  1.54s/it]Epoch 1/1:   8%|▊         | 6/80 [00:12<01:38,  1.34s/it]Epoch 1/1:   9%|▉         | 7/80 [00:13<01:34,  1.30s/it]Epoch 1/1:  10%|█         | 8/80 [00:14<01:21,  1.13s/it]Epoch 1/1:  11%|█▏        | 9/80 [00:14<01:05,  1.08it/s]Epoch 1/1:  12%|█▎        | 10/80 [00:15<01:00,  1.17it/s]Epoch 1/1:  14%|█▍        | 11/80 [00:16<01:12,  1.05s/it]Epoch 1/1:  15%|█▌        | 12/80 [00:17<00:57,  1.18it/s]Epoch 1/1:  16%|█▋        | 13/80 [00:18<00:59,  1.13it/s]Epoch 1/1:  18%|█▊        | 14/80 [00:19<01:06,  1.00s/it]Epoch 1/1:  19%|█▉        | 15/80 [00:20<01:02,  1.04it/s]Epoch 1/1:  20%|██        | 16/80 [00:20<00:54,  1.18it/s]Epoch 1/1:  21%|██▏       | 17/80 [00:21<00:43,  1.44it/s]Epoch 1/1:  22%|██▎       | 18/80 [00:21<00:44,  1.38it/s]Epoch 1/1:  24%|██▍       | 19/80 [00:22<00:35,  1.71it/s]Epoch 1/1:  25%|██▌       | 20/80 [00:23<00:49,  1.22it/s]Epoch 1/1:  26%|██▋       | 21/80 [00:24<00:47,  1.24it/s]Epoch 1/1:  28%|██▊       | 22/80 [00:24<00:34,  1.66it/s]Epoch 1/1:  29%|██▉       | 23/80 [00:25<00:35,  1.61it/s]Epoch 1/1:  30%|███       | 24/80 [00:25<00:26,  2.08it/s]Epoch 1/1:  31%|███▏      | 25/80 [00:26<00:39,  1.38it/s]Epoch 1/1:  32%|███▎      | 26/80 [00:27<00:43,  1.23it/s]Epoch 1/1:  34%|███▍      | 27/80 [00:28<00:42,  1.26it/s]Epoch 1/1:  35%|███▌      | 28/80 [00:28<00:38,  1.36it/s]Epoch 1/1:  36%|███▋      | 29/80 [00:30<00:44,  1.15it/s]Epoch 1/1:  38%|███▊      | 30/80 [00:31<00:47,  1.04it/s]Epoch 1/1:  39%|███▉      | 31/80 [00:31<00:42,  1.16it/s]Epoch [1/1], UpdateStep [1/3], LR: 3.75e-05, Loss: 0.0187, Acc(token): 0.7016
Epoch 1/1:  40%|████      | 32/80 [00:36<01:39,  2.07s/it]Epoch 1/1:  41%|████▏     | 33/80 [00:37<01:14,  1.59s/it]Epoch 1/1:  42%|████▎     | 34/80 [00:38<01:06,  1.45s/it]Epoch 1/1:  44%|████▍     | 35/80 [00:39<01:01,  1.37s/it]Epoch 1/1:  45%|████▌     | 36/80 [00:39<00:44,  1.02s/it]Epoch 1/1:  46%|████▋     | 37/80 [00:40<00:40,  1.06it/s]Epoch 1/1:  48%|████▊     | 38/80 [00:40<00:32,  1.30it/s]Epoch 1/1:  49%|████▉     | 39/80 [00:42<00:40,  1.01it/s]Epoch 1/1:  50%|█████     | 40/80 [00:43<00:41,  1.04s/it]Epoch 1/1:  51%|█████▏    | 41/80 [00:44<00:41,  1.05s/it]Epoch 1/1:  52%|█████▎    | 42/80 [00:44<00:31,  1.20it/s]Epoch 1/1:  54%|█████▍    | 43/80 [00:46<00:35,  1.03it/s]Epoch 1/1:  55%|█████▌    | 44/80 [00:47<00:33,  1.07it/s]Epoch 1/1:  56%|█████▋    | 45/80 [00:48<00:33,  1.05it/s]Epoch 1/1:  57%|█████▊    | 46/80 [00:49<00:36,  1.07s/it]Epoch 1/1:  59%|█████▉    | 47/80 [00:49<00:26,  1.24it/s]Epoch 1/1:  60%|██████    | 48/80 [00:50<00:26,  1.19it/s]Epoch 1/1:  61%|██████▏   | 49/80 [00:51<00:27,  1.14it/s]Epoch 1/1:  62%|██████▎   | 50/80 [00:52<00:28,  1.05it/s]Epoch 1/1:  64%|██████▍   | 51/80 [00:53<00:27,  1.07it/s]Epoch 1/1:  65%|██████▌   | 52/80 [00:54<00:25,  1.12it/s]Epoch 1/1:  66%|██████▋   | 53/80 [00:54<00:17,  1.52it/s]Epoch 1/1:  68%|██████▊   | 54/80 [00:54<00:13,  1.94it/s]Epoch 1/1:  69%|██████▉   | 55/80 [00:54<00:09,  2.51it/s]Epoch 1/1:  70%|███████   | 56/80 [00:56<00:16,  1.43it/s]Epoch 1/1:  71%|███████▏  | 57/80 [00:56<00:12,  1.79it/s]Epoch 1/1:  72%|███████▎  | 58/80 [00:56<00:10,  2.12it/s]Epoch 1/1:  74%|███████▍  | 59/80 [00:56<00:07,  2.64it/s]Epoch 1/1:  75%|███████▌  | 60/80 [00:57<00:09,  2.05it/s]Epoch 1/1:  76%|███████▋  | 61/80 [00:58<00:10,  1.86it/s]Epoch 1/1:  78%|███████▊  | 62/80 [00:58<00:07,  2.42it/s]Epoch 1/1:  79%|███████▉  | 63/80 [00:58<00:06,  2.45it/s]Epoch [1/1], UpdateStep [2/3], LR: 1.25e-05, Loss: 0.0164, Acc(token): 0.7875
Epoch 1/1:  81%|████████▏ | 65/80 [01:00<00:08,  1.79it/s]Epoch 1/1:  82%|████████▎ | 66/80 [01:01<00:09,  1.54it/s]Epoch 1/1:  84%|████████▍ | 67/80 [01:01<00:07,  1.70it/s]Epoch 1/1:  85%|████████▌ | 68/80 [01:02<00:08,  1.36it/s]Epoch 1/1:  86%|████████▋ | 69/80 [01:02<00:06,  1.68it/s]Epoch 1/1:  88%|████████▊ | 70/80 [01:03<00:04,  2.02it/s]Epoch 1/1:  89%|████████▉ | 71/80 [01:03<00:04,  1.82it/s]Epoch 1/1:  90%|█████████ | 72/80 [01:04<00:05,  1.57it/s]Epoch 1/1:  91%|█████████▏| 73/80 [01:05<00:04,  1.72it/s]Epoch 1/1:  92%|█████████▎| 74/80 [01:06<00:04,  1.38it/s]Epoch 1/1:  94%|█████████▍| 75/80 [01:06<00:02,  1.68it/s]Epoch 1/1:  95%|█████████▌| 76/80 [01:06<00:01,  2.18it/s]Epoch 1/1:  96%|█████████▋| 77/80 [01:06<00:01,  2.82it/s]Epoch 1/1:  98%|█████████▊| 78/80 [01:07<00:00,  2.39it/s]Epoch 1/1:  99%|█████████▉| 79/80 [01:07<00:00,  2.08it/s]Epoch 1/1: 100%|██████████| 80/80 [01:08<00:00,  1.92it/s]Epoch 1/1: 100%|██████████| 80/80 [01:08<00:00,  1.16it/s]
Saved checkpoint: ckpts/seval_qwen3_8b_sample/model_epoch_0.pt
Training complete!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:05,  1.39s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.31s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:05<00:07,  2.57s/it]
Traceback (most recent call last):
  File "/home/ruqi/public/Kelp/train.py", line 345, in <module>
    main()  # 调用主函数
    ~~~~^^
  File "/home/ruqi/public/Kelp/train.py", line 341, in main
    train(args)  # 调用训练函数，传入解析后的参数
    ~~~~~^^^^^^
  File "/home/ruqi/public/Kelp/train.py", line 248, in train
    predictions, references = evaluate_safety_head(  # 调用评估函数，评估训练好的模型
                              ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        ckpt_path=ckpt_path,  # 检查点路径（最后一个epoch的模型）
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        bf16=True  # 启用bfloat16混合精度
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ruqi/public/Kelp/eval.py", line 29, in evaluate_safety_head
    base_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=device
    )
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        model,
        ^^^^^^
    ...<12 lines>...
        weights_only=weights_only,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5468, in _load_pretrained_model
    _error_msgs, disk_offload_index = load_shard_file(args)
                                      ~~~~~~~~~~~~~~~^^^^^^
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 843, in load_shard_file
    disk_offload_index = _load_state_dict_into_meta_model(
        model,
    ...<8 lines>...
        device_mesh=device_mesh,
    )
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/u/r/u/ruqi/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py", line 770, in _load_state_dict_into_meta_model
    _load_parameter_into_model(model, param_name, param.to(param_device))
                                                  ~~~~~~~~^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 24.69 MiB is free. Including non-PyTorch memory, this process has 10.72 GiB memory in use. Of the allocated memory 10.47 GiB is allocated by PyTorch, and 60.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
