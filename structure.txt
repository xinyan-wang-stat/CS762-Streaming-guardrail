train.py::main()
  └─> train.py::train(args)
       ├─> 加载 base_model 和 tokenizer
       ├─> SafetyDataset.__init__()  [来自 dataset.py]
       │    └─> SafetyDataset._build_cache_per_sample()  [如果缓存不存在]
       │         └─> load_from_disk()  [加载原始数据集]
       │         └─> base_model.generate()  [提取隐藏状态]
       │
       ├─> StreamingSafetyHead()  [来自 models.py]
       │
       ├─> 训练循环
       │    └─> DataLoader 迭代 SafetyDataset
       │    └─> safety_head.forward()  [来自 models.py]
       │
       └─> eval.py::evaluate_safety_head()  [训练结束后自动评估]
            ├─> SafetyDataset.__init__()  [加载测试集]
            └─> StreamingSafetyHead()  [加载训练好的模型]


# model.py 
输入: x (B, seq_len, input_dim=4096)
        ↓
[1] AttentionLayer
        ↓
    输出: feat (B, seq_len, proj_dim=512)
        ↓
    ├─ 用户前缀部分 (B, user_len, proj_dim) ────┐
    └─ 助手序列部分 (B, assist_len, proj_dim)   │
        ↓                                        │
    [2] 前缀初始化阶段                            │
        ├─ prefix_scorer ──→ scores              │
        ├─ softmax ──→ weights                   │
        ├─ 加权池化 ──→ pooled (B, proj_dim)     │
        └─ prefix_to_h ──→ 初始隐藏状态 h_0     │
        ↓                                        │
    [3] 流式处理阶段 (对每个token) ◄───────────────┘
        ├─ CfcCell (维护记忆状态 h_t)
        └─ mem_head ──→ logits_t
        ↓
输出: logits (B, assist_len, num_labels=2)


1. AttentionLayer（注意力层）
输入: x (B, seq_len, input_dim=4096)
    ↓
Query/Key/Value 线性投影:
    - Q = Linear(4096 → 512)
    - K = Linear(4096 → 512)  
    - V = Linear(4096 → 512)
    ↓
自注意力计算:
    - attention_scores = QK^T / sqrt(d_k)
    - 应用掩码（因果掩码或自定义掩码）
    - attention_weights = softmax(attention_scores)
    - context_vector = attention_weights @ V
    ↓
输出: context_vector (B, seq_len, proj_dim=512)

2. 前缀初始化模块
输入: user_hidden (B, user_len, proj_dim=512)
    ↓
[2.1] 前缀评分器:
    - scores = prefix_scorer(user_hidden)  # Linear(512 → 1)
    - 输出: (B, user_len)
    ↓
[2.2] 注意力权重计算:
    - weights = softmax(scores, dim=1)
    - 输出: (B, user_len)  # 权重和为1
    ↓
[2.3] 加权池化:
    - pooled = weights @ user_hidden
    - 输出: (B, proj_dim=512)
    ↓
[2.4] 前缀到隐藏状态转换:
    - prefix_to_h = Sequential(
        Linear(512 → 512),
        Tanh()
      )
    - h_0 = prefix_to_h(pooled)
    ↓
输出: 初始隐藏状态 h_0 (B, mem_dim=512)

3. CfcCell（循环单元结构）
CfcCell 内部结构:
    ├─ 更新门 (Update Gate):
    │   z = sigmoid(x @ Wz + h_prev @ Uz + bz)
    │
    ├─ 重置门 (Reset Gate):
    │   r = sigmoid(x @ Wr + h_prev @ Ur + br)
    │
    ├─ 候选隐藏状态:
    │   h_hat = tanh(x @ Wh + (r ⊙ h_prev) @ Uh + bh)
    │
    └─ 新隐藏状态:
        h_new = (1-z) ⊙ h_prev + z ⊙ h_hat
        h_final = h_new + dt * (h_new - h_prev)  # 连续时间项
参数矩阵：
Wz, Uz, bz: 更新门参数
Wr, Ur, br: 重置门参数
Wh, Uh, bh: 候选状态参数
所有矩阵都使用正交初始化
4. 流式处理循环
输入: 
    - feat (B, seq_total, proj_dim=512)
    - assistant_start (int)
    ↓
初始化:
    - prefix = feat[:, :assistant_start, :]  # 用户前缀
    - init_with_prefix(assist_len, prefix)  # 初始化 h_0 和 dt
    ↓
循环处理 (t = 0 to assist_len-1):
    for t in range(assist_len):
        x_t = feat[:, assistant_start+t, :]  # (B, proj_dim)
        ↓
        [3.1] CfcCell 更新:
            h_t = CfcCell(x_t, h_{t-1}, dt[t])
            输出: h_t (B, mem_dim=512)
        ↓
        [3.2] 分类头:
            logits_t = mem_head(h_t)  # Linear(512 → 2)
            输出: logits_t (B, num_labels=2)
        ↓
拼接所有时刻:
    logits = [logits_0, logits_1, ..., logits_{assist_len-1}]
    ↓
输出: logits (B, assist_len, num_labels=2)


维度变化流程图

输入维度流:
(B, seq_len, 4096)  [基础模型的嵌入向量]
        ↓ [AttentionLayer]
(B, seq_len, 512)   [投影后的特征]
        ↓ [分离]
├─ (B, user_len, 512)  [用户前缀] ─→ [前缀处理] ─→ (B, 512) [初始隐藏状态]
└─ (B, assist_len, 512) [助手序列]
        ↓ [逐token处理]
    t=0: (B, 512) ─→ CfcCell ─→ (B, 512) [h_0] ─→ Linear ─→ (B, 2) [logits_0]
    t=1: (B, 512) ─→ CfcCell ─→ (B, 512) [h_1] ─→ Linear ─→ (B, 2) [logits_1]
    ...
    t=T: (B, 512) ─→ CfcCell ─→ (B, 512) [h_T] ─→ Linear ─→ (B, 2) [logits_T]
        ↓ [拼接]
(B, assist_len, 2)  [最终输出：每个token的安全分类logits]